{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import sklearn\n",
    "import kaggle\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset from Kaggle\n",
    "# This will download the dataset to a local cache directory\n",
    "dataset_path = kagglehub.dataset_download(\"pratyushpuri/wearable-health-devices-performance-analysis\")\n",
    "\n",
    "print(f\"Dataset downloaded to: {dataset_path}\")\n",
    "\n",
    "# List files in the downloaded dataset\n",
    "import os\n",
    "dataset_files = os.listdir(dataset_path)\n",
    "print(f\"Files in dataset: {dataset_files}\")\n",
    "\n",
    "# Load the main CSV file (assuming there's a CSV file in the dataset)\n",
    "# You may need to adjust the filename based on what's actually in the dataset\n",
    "csv_files = [f for f in dataset_files if f.endswith('.csv')]\n",
    "if csv_files:\n",
    "    main_file = csv_files[0]  # Use the first CSV file found\n",
    "    df = pd.read_csv(os.path.join(dataset_path, main_file))\n",
    "    print(f\"Loaded file: {main_file}\")\n",
    "    print(\"First 5 records:\", df.head())\n",
    "else:\n",
    "    print(\"No CSV files found. Available files:\", dataset_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Exploratory Data Analysis (EDA)\n",
    "# First, let's properly load the dataset\n",
    "\n",
    "# Download the dataset from Kaggle\n",
    "dataset_path = kagglehub.dataset_download(\"pratyushpuri/wearable-health-devices-performance-analysis\")\n",
    "print(f\"Dataset downloaded to: {dataset_path}\")\n",
    "\n",
    "# List and load the dataset files\n",
    "dataset_files = os.listdir(dataset_path)\n",
    "print(f\"Files in dataset: {dataset_files}\")\n",
    "\n",
    "# Load the CSV file\n",
    "csv_files = [f for f in dataset_files if f.endswith('.csv')]\n",
    "if csv_files:\n",
    "    main_file = csv_files[0]\n",
    "    df = pd.read_csv(os.path.join(dataset_path, main_file))\n",
    "    print(f\"\\nLoaded file: {main_file}\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "else:\n",
    "    print(\"No CSV files found. Available files:\", dataset_files)\n",
    "\n",
    "# Basic Dataset Information\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BASIC DATASET INFORMATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"Number of rows: {df.shape[0]:,}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")\n",
    "\n",
    "print(\"\\nColumn Names and Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nLast 5 rows:\")\n",
    "display(df.tail())\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "df.info()\n",
    "\n",
    "# Missing Values Analysis\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing Count': missing_values,\n",
    "    'Missing Percentage': missing_percentage\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "\n",
    "print(\"Missing values summary:\")\n",
    "display(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "if missing_df['Missing Count'].sum() == 0:\n",
    "    print(\"âœ… No missing values found in the dataset!\")\n",
    "\n",
    "# Statistical Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nDescriptive statistics for numerical columns:\")\n",
    "display(df.describe())\n",
    "\n",
    "print(\"\\nDescriptive statistics for categorical columns:\")\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "if len(categorical_cols) > 0:\n",
    "    display(df[categorical_cols].describe())\n",
    "else:\n",
    "    print(\"No categorical columns found.\")\n",
    "\n",
    "# Data Distribution Analysis\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumerical columns ({len(numerical_cols)}): {numerical_cols}\")\n",
    "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "\n",
    "# Visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# 1. Distribution of numerical variables\n",
    "if len(numerical_cols) > 0:\n",
    "    n_cols = min(3, len(numerical_cols))\n",
    "    n_rows = (len(numerical_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = [axes] if n_cols == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        if i < len(axes):\n",
    "            sns.histplot(data=df, x=col, kde=True, ax=axes[i])\n",
    "            axes[i].set_title(f'Distribution of {col}')\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(numerical_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 2. Box plots for numerical variables\n",
    "if len(numerical_cols) > 0:\n",
    "    n_cols = min(3, len(numerical_cols))\n",
    "    n_rows = (len(numerical_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = [axes] if n_cols == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        if i < len(axes):\n",
    "            sns.boxplot(data=df, y=col, ax=axes[i])\n",
    "            axes[i].set_title(f'Box Plot of {col}')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(numerical_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 3. Categorical variables analysis\n",
    "if len(categorical_cols) > 0:\n",
    "    for col in categorical_cols:\n",
    "        print(f\"\\nValue counts for {col}:\")\n",
    "        value_counts = df[col].value_counts()\n",
    "        print(value_counts)\n",
    "        \n",
    "        # Plot categorical distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        if len(value_counts) <= 20:  # Only plot if not too many categories\n",
    "            sns.countplot(data=df, x=col)\n",
    "            plt.title(f'Distribution of {col}')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Too many categories ({len(value_counts)}) to plot effectively.\")\n",
    "\n",
    "# 4. Correlation Analysis\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if len(numerical_cols) > 1:\n",
    "    correlation_matrix = df[numerical_cols].corr()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, linewidths=0.5)\n",
    "    plt.title('Correlation Matrix of Numerical Variables')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find highly correlated pairs\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_val = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.7:  # High correlation threshold\n",
    "                high_corr_pairs.append((\n",
    "                    correlation_matrix.columns[i], \n",
    "                    correlation_matrix.columns[j], \n",
    "                    corr_val\n",
    "                ))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print(\"\\nHighly correlated pairs (|correlation| > 0.7):\")\n",
    "        for var1, var2, corr in high_corr_pairs:\n",
    "            print(f\"{var1} - {var2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(\"\\nNo highly correlated pairs found (|correlation| > 0.7)\")\n",
    "\n",
    "# 5. Outlier Detection\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"OUTLIER DETECTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if len(numerical_cols) > 0:\n",
    "    outlier_summary = []\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        outlier_count = len(outliers)\n",
    "        outlier_percentage = (outlier_count / len(df)) * 100\n",
    "        \n",
    "        outlier_summary.append({\n",
    "            'Column': col,\n",
    "            'Outlier Count': outlier_count,\n",
    "            'Outlier Percentage': outlier_percentage,\n",
    "            'Lower Bound': lower_bound,\n",
    "            'Upper Bound': upper_bound\n",
    "        })\n",
    "    \n",
    "    outlier_df = pd.DataFrame(outlier_summary)\n",
    "    print(\"Outlier summary:\")\n",
    "    display(outlier_df)\n",
    "\n",
    "# 6. Data Quality Assessment\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check for duplicates\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"\\nDuplicate rows: {duplicate_count} ({(duplicate_count/len(df)*100):.2f}%)\")\n",
    "\n",
    "# Check for constant columns\n",
    "constant_cols = [col for col in df.columns if df[col].nunique() <= 1]\n",
    "if constant_cols:\n",
    "    print(f\"\\nConstant columns (single unique value): {constant_cols}\")\n",
    "else:\n",
    "    print(\"\\nâœ… No constant columns found.\")\n",
    "\n",
    "# Unique values per column\n",
    "print(\"\\nUnique values per column:\")\n",
    "unique_counts = df.nunique().sort_values(ascending=False)\n",
    "display(pd.DataFrame({'Column': unique_counts.index, 'Unique Values': unique_counts.values}))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EDA COMPLETE!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
